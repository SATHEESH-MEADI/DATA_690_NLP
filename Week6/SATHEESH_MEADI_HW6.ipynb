{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1H0ngbWK2V9T_mI2rrOiX4ojY0ZhMWRfU","authorship_tag":"ABX9TyPPqYmyHqk2kvJ8BbkrsGvq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Question:1"],"metadata":{"id":"McEG3IP-oRj1"}},{"cell_type":"markdown","source":["Exercise 1. Text Generation\n","- Install markovify\n","- Import pandas and markovify\n","- Load the file ‘abcnews-date-text.csc’ as ‘inp’\n","- Look at the three top rows\n","- Create a model with markovify as ‘text_model’ to generate text\n","- Print ten randomly generated sentences using the built model."],"metadata":{"id":"As9RsqF9By7l"}},{"cell_type":"markdown","source":["# Installing required packages"],"metadata":{"id":"gu4D5FQo_Q9z"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m97yFIkbZWjL","executionInfo":{"status":"ok","timestamp":1729238097731,"user_tz":240,"elapsed":5421,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"5dbe55f2-9da0-4ccc-bc34-e932ef778361"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: markovify in /usr/local/lib/python3.10/dist-packages (0.9.4)\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from markovify) (1.3.8)\n"]}],"source":["pip install markovify"]},{"cell_type":"markdown","source":["Importing the Markovify and pandas"],"metadata":{"id":"4myjDGMO_ZYz"}},{"cell_type":"code","source":["import pandas as pd\n","import markovify"],"metadata":{"id":"x-7Bx9-WaKXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mounting the drive if needed, In my case it automatically connected to drive this time."],"metadata":{"id":"DWqvoOLP_qdn"}},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"QuuzhURHbACw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reading the data from the drive and printing 3 rows of data for verification."],"metadata":{"id":"Uwrdw7mQ_37E"}},{"cell_type":"code","source":["                                                                                # Load the file\n","inp = pd.read_csv('/content/drive/MyDrive/NLP/Week6/abcnews-date-text.csv')\n","\n","                                                                                # Look at the top 3 rows\n","print(inp.head(3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kjvUb2ll2xq","executionInfo":{"status":"ok","timestamp":1729238100415,"user_tz":240,"elapsed":2688,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"13b3a84e-b1ef-4947-8c7f-c108e7f68e4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   publish_date                                      headline_text\n","0      20030219  aba decides against community broadcasting lic...\n","1      20030219     act fire witnesses must be aware of defamation\n","2      20030219     a g calls for infrastructure protection summit\n"]}]},{"cell_type":"markdown","source":["Creating a model by giving the input as the text from input file."],"metadata":{"id":"_r8GD0I4AGsj"}},{"cell_type":"code","source":["                                                                                # Create a model\n","text_model = markovify.Text(inp['headline_text'])"],"metadata":{"id":"eUNyDa5FllWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trying to generate 10 random sentences using the model."],"metadata":{"id":"l6fJ3R0jAyJb"}},{"cell_type":"code","source":["                                                                                # Generate and print ten randomly generated sentences using the built model\n","generated_sentences = [text_model.make_sentence() for _ in range(10)]\n","generated_sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eyOe6QesnX_L","executionInfo":{"status":"ok","timestamp":1729238161413,"user_tz":240,"elapsed":829,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"2598cfaf-3035-4807-ab03-f495a5385c92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['flynn wants court appeals',\n"," 'pricey coffee good to go to flood buckland park development stops',\n"," 'market closes lower as a way of life',\n"," 'how to suck up godfreys',\n"," 'dave mcrae gives an update on dreamworld tragedy',\n"," 'australian markets follow wall st to fresh 11 month high',\n"," 'injured bond to miss geelong clash',\n"," 'ernest fisher riddled with asbestos',\n"," 'recycling rush e waste ending up in central pakistan explosion',\n"," 'antarctic health hazards blamed on fuel starved motorists']"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["# Question:2"],"metadata":{"id":"zEsz1ihQoNrv"}},{"cell_type":"markdown","source":["Exercise 2. Text Summarization\n","- Use sumy to summarize the ‘alice.txt’ file\n","- Download the ‘punkt’ and 'tokenizers/punkt/PY3/english.pickle' NLTK\n","libraries.\n","\n"],"metadata":{"id":"IDs0QCXWovsA"}},{"cell_type":"markdown","source":["# Importing and downloading the required libraries"],"metadata":{"id":"UOP92OpcBoOz"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('tokenizers/punkt/PY3/english.pickle')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLaQz5LEpL3-","executionInfo":{"status":"ok","timestamp":1729239032105,"user_tz":240,"elapsed":187,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"92750ce3-2bbf-4d47-fff9-e642a832164f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Error loading tokenizers/punkt/PY3/english.pickle: Package\n","[nltk_data]     'tokenizers/punkt/PY3/english.pickle' not found in\n","[nltk_data]     index\n"]},{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["pip install sumy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjfNb5Q5pXcz","executionInfo":{"status":"ok","timestamp":1729239037262,"user_tz":240,"elapsed":3336,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"a58f8def-d713-4e5a-e044-e0a65879aedb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n","Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n","Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n","Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (24.6.1)\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n"]}]},{"cell_type":"code","source":["import sumy\n","from sumy.summarizers.lsa import LsaSummarizer\n","from sumy.parsers.plaintext import PlaintextParser\n","from sumy.nlp.tokenizers import Tokenizer\n","import os"],"metadata":{"id":"1-4d_Rt6pNRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing the data from the text file \"alice.txt\" which is in the drive"],"metadata":{"id":"HYE89nQfCFyq"}},{"cell_type":"code","source":["file_path = \"/content/drive/MyDrive/NLP/Week6/alice.txt\"\n","with open(file_path, 'r', encoding='utf-8') as f:\n","  text = f.read()"],"metadata":{"id":"BU8-7hC8oM-E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sending the input text data to a parser"],"metadata":{"id":"5qUtIVBzCawp"}},{"cell_type":"code","source":["parser = PlaintextParser(text, Tokenizer(\"english\"))\n","parser"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHoE7FOXqpWL","executionInfo":{"status":"ok","timestamp":1729239043047,"user_tz":240,"elapsed":276,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"01aef5c6-3a23-4c1b-d7e9-3193ab092256"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sumy.parsers.plaintext.PlaintextParser at 0x7fd7bd8238e0>"]},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","source":["# Trying to summarize the input text data into 5 sentences"],"metadata":{"id":"aJrBO04QCix-"}},{"cell_type":"code","source":["summarizer = LsaSummarizer()\n","summary = summarizer(parser.document, 5)                                        # Summarize to 5 sentences"],"metadata":{"id":"4uK7iGgFqv2C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Printing those generated summarized 5 sentences."],"metadata":{"id":"PmzLCKAvCvJG"}},{"cell_type":"code","source":["for sentence in summary:\n","    print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8awyBynlrmTZ","executionInfo":{"status":"ok","timestamp":1729239089919,"user_tz":240,"elapsed":201,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"c9a9d039-14e6-439b-feb3-128f2ef285b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The Mouse did not answer, so Alice went on eagerly:  `There is such a nice little dog near our house I should like to show you!\n","The poor little thing sobbed again (or grunted, it was impossible to say which), and they went on for some while in silence.\n","He sent them word I had not gone (We know it to be true): If she should push the matter on, What would become of you?\n","Don't let him know she liked them best, For this must ever be A secret, kept from all the rest, Between yourself and me.'\n","`If there's no meaning in it,' said the King, `that saves a world of trouble, you know, as we needn't try to find any.\n"]}]},{"cell_type":"markdown","source":["# Question:3"],"metadata":{"id":"KRwX9hfzrqXJ"}},{"cell_type":"markdown","source":["Exercise 3. Topic Modeling\n","- Determine the top 20 topics using the Non-Negative Matrix\n","Factorization (NMF) using ‘from sklearn.decomposition import NMF’\n","- Vectorize the words after cleaning up the text\n","- Use ‘print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in idx_to_word\n","[topic.argsort()[-10:]]]))) to list the topics"],"metadata":{"id":"k4E-htIIC_zD"}},{"cell_type":"markdown","source":["# Installing and importing the required libraries"],"metadata":{"id":"Wa2eFUabDgnt"}},{"cell_type":"code","source":["pip install scikit-learn pandas numpy nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9U-4IkPIsVGl","executionInfo":{"status":"ok","timestamp":1729239280707,"user_tz":240,"elapsed":3608,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"e7838f9f-6fdb-4752-bde8-dbc818b38884"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}]},{"cell_type":"code","source":["from sklearn.decomposition import NMF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np"],"metadata":{"id":"9SVPG3OtsX9M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extracting the data from the csv file which is in the drive"],"metadata":{"id":"KhoE46upDoFo"}},{"cell_type":"code","source":["                                                                                # Load the CSV file from your drive\n","file_path = '/content/drive/MyDrive/NLP/Week6/bbc_text.csv'\n","df = pd.read_csv(file_path)"],"metadata":{"id":"Rzd6Dmhksbc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["                                                                                # Extract the column containing text data\n","documents = df['text'].dropna().tolist()"],"metadata":{"id":"Nw736U93wbye"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# To create a TF-IDF matrix for documents, ignoring English stop words, and limits features to 1000 unique words."],"metadata":{"id":"CmBLLDF6ENFO"}},{"cell_type":"code","source":["vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n","tfidf_matrix = vectorizer.fit_transform(documents)\n","idx_to_word = np.array(vectorizer.get_feature_names_out())"],"metadata":{"id":"sqS-tPuqsevo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# To create an NMF model with 20 components and fits it to transform the `tfidf_matrix` into topic features."],"metadata":{"id":"_Qzrhfl-E2Nf"}},{"cell_type":"code","source":["nmf_model = NMF(n_components=20, random_state=42)\n","nmf_features = nmf_model.fit_transform(tfidf_matrix)"],"metadata":{"id":"syP17nYLshJA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# To print the output as specified in the question"],"metadata":{"id":"MuW8RAWnE-aF"}},{"cell_type":"code","source":["for i, topic in enumerate(nmf_model.components_):\n","    print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in idx_to_word[topic.argsort()[-10:]]])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MB9-RofqsjoL","executionInfo":{"status":"ok","timestamp":1729240829473,"user_tz":240,"elapsed":207,"user":{"displayName":"Satheesh Meadi","userId":"06653658269404937466"}},"outputId":"e75c35da-62e7-4391-e2ce-a82424661d03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 1: report,plans,children,local,work,public,000,people,said,government\n","Topic 2: leader,kennedy,lib,tories,tory,tax,howard,election,party,labour\n","Topic 3: time,players,like,sony,play,titles,video,gaming,game,games\n","Topic 4: hollywood,star,box,actor,oscar,director,movie,films,festival,film\n","Topic 5: sony,video,devices,content,media,apple,tv,technology,digital,music\n","Topic 6: bid,offer,financial,said,deal,stock,market,firm,company,shares\n","Topic 7: half,scotland,coach,nations,robinson,rugby,france,ireland,wales,england\n","Topic 8: manchester,manager,season,cup,liverpool,league,united,arsenal,chelsea,club\n","Topic 9: tour,single,song,singer,chart,number,rock,music,album,band\n","Topic 10: second,round,roddick,set,beat,final,win,australian,match,open\n","Topic 11: search,mail,programs,windows,computer,security,users,virus,microsoft,software\n","Topic 12: sale,state,unit,bankruptcy,tax,court,russia,russian,oil,yukos\n","Topic 13: networks,data,camera,people,technology,use,mobiles,phones,phone,mobile\n","Topic 14: told,tony,election,labour,chancellor,minister,prime,brown,blair,mr\n","Topic 15: exports,china,dollar,prices,rates,rate,bank,economic,growth,economy\n","Topic 16: actor,actress,named,ceremony,nominations,prize,won,award,awards,best\n","Topic 17: human,mr,court,rights,police,lords,lord,said,eu,law\n","Topic 18: european,year,record,women,gold,champion,athens,world,race,olympic\n","Topic 19: car,profit,figures,christmas,december,rose,year,profits,2004,sales\n","Topic 20: tv,users,internet,online,uk,people,service,net,bt,broadband\n"]}]}]}